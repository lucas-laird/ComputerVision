%!PS-Adobe-3.0
%%Title: linear_classifier.py
%%For: root
%%Creator: VIM - Vi IMproved 7.4 (2013 Aug 10)
%%CreationDate: Sun May  6 11:53:54 2018
%%DocumentData: Clean8Bit
%%Orientation: Portrait
%%Pages: (atend)
%%PageOrder: Ascend
%%BoundingBox: 59 49 564 800
%%DocumentMedia: A4 595 842 0 () ()
%%DocumentNeededResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%DocumentSuppliedResources: procset VIM-Prolog 1.4 1
%%+ encoding VIM-latin1 1.0 0
%%Requirements: duplex collate color
%%EndComments
%%BeginDefaults
%%PageResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%PageMedia: A4
%%EndDefaults
%%BeginProlog
%%BeginResource: procset VIM-Prolog
%%BeginDocument: /usr/share/vim/vim74/print/prolog.ps
%!PS-Adobe-3.0 Resource-ProcSet
%%Title: VIM-Prolog
%%Version: 1.4 1
%%EndComments
% Editing of this file is NOT RECOMMENDED.  You run a very good risk of causing
% all PostScript printing from VIM failing if you do.  PostScript is not called
% a write-only language for nothing!
/packedarray where not{userdict begin/setpacking/pop load def/currentpacking
false def end}{pop}ifelse/CP currentpacking def true setpacking
/bd{bind def}bind def/ld{load def}bd/ed{exch def}bd/d/def ld
/db{dict begin}bd/cde{currentdict end}bd
/T true d/F false d
/SO null d/sv{/SO save d}bd/re{SO restore}bd
/L2 systemdict/languagelevel 2 copy known{get exec}{pop pop 1}ifelse 2 ge d
/m/moveto ld/s/show ld /ms{m s}bd /g/setgray ld/r/setrgbcolor ld/sp{showpage}bd
/gs/gsave ld/gr/grestore ld/cp/currentpoint ld
/ul{gs UW setlinewidth cp UO add 2 copy newpath m 3 1 roll add exch lineto
stroke gr}bd
/bg{gs r cp BO add 4 -2 roll rectfill gr}bd
/sl{90 rotate 0 exch translate}bd
L2{
/sspd{mark exch{setpagedevice}stopped cleartomark}bd
/nc{1 db/NumCopies ed cde sspd}bd
/sps{3 db/Orientation ed[3 1 roll]/PageSize ed/ImagingBBox null d cde sspd}bd
/dt{2 db/Tumble ed/Duplex ed cde sspd}bd
/c{1 db/Collate ed cde sspd}bd
}{
/nc{/#copies ed}bd
/sps{statusdict/setpage get exec}bd
/dt{statusdict/settumble 2 copy known{get exec}{pop pop pop}ifelse
statusdict/setduplexmode 2 copy known{get exec}{pop pop pop}ifelse}bd
/c{pop}bd
}ifelse
/ffs{findfont exch scalefont d}bd/sf{setfont}bd
/ref{1 db findfont dup maxlength dict/NFD ed{exch dup/FID ne{exch NFD 3 1 roll
put}{pop pop}ifelse}forall/Encoding findresource dup length 256 eq{NFD/Encoding
3 -1 roll put}{pop}ifelse NFD dup/FontType get 3 ne{/CharStrings}{/CharProcs}
ifelse 2 copy known{2 copy get dup maxlength dict copy[/questiondown/space]{2
copy known{2 copy get 2 index/.notdef 3 -1 roll put pop exit}if pop}forall put
}{pop pop}ifelse dup NFD/FontName 3 -1 roll put NFD definefont pop end}bd
CP setpacking
(\004)cvn{}bd
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%BeginResource: encoding VIM-latin1
%%BeginDocument: /usr/share/vim/vim74/print/latin1.ps
%!PS-Adobe-3.0 Resource-Encoding
%%Title: VIM-latin1
%%Version: 1.0 0
%%EndComments
/VIM-latin1[
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclam /quotedbl /numbersign /dollar /percent /ampersand /quotesingle
/parenleft /parenright /asterisk /plus /comma /minus /period /slash
/zero /one /two /three /four /five /six /seven
/eight /nine /colon /semicolon /less /equal /greater /question
/at /A /B /C /D /E /F /G
/H /I /J /K /L /M /N /O
/P /Q /R /S /T /U /V /W
/X /Y /Z /bracketleft /backslash /bracketright /asciicircum /underscore
/grave /a /b /c /d /e /f /g
/h /i /j /k /l /m /n /o
/p /q /r /s /t /u /v /w
/x /y /z /braceleft /bar /braceright /asciitilde /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclamdown /cent /sterling /currency /yen /brokenbar /section
/dieresis /copyright /ordfeminine /guillemotleft /logicalnot /hyphen /registered /macron
/degree /plusminus /twosuperior /threesuperior /acute /mu /paragraph /periodcentered
/cedilla /onesuperior /ordmasculine /guillemotright /onequarter /onehalf /threequarters /questiondown
/Agrave /Aacute /Acircumflex /Atilde /Adieresis /Aring /AE /Ccedilla
/Egrave /Eacute /Ecircumflex /Edieresis /Igrave /Iacute /Icircumflex /Idieresis
/Eth /Ntilde /Ograve /Oacute /Ocircumflex /Otilde /Odieresis /multiply
/Oslash /Ugrave /Uacute /Ucircumflex /Udieresis /Yacute /Thorn /germandbls
/agrave /aacute /acircumflex /atilde /adieresis /aring /ae /ccedilla
/egrave /eacute /ecircumflex /edieresis /igrave /iacute /icircumflex /idieresis
/eth /ntilde /ograve /oacute /ocircumflex /otilde /odieresis /divide
/oslash /ugrave /uacute /ucircumflex /udieresis /yacute /thorn /ydieresis]
/Encoding defineresource pop
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%EndProlog
%%BeginSetup
595 842 0 sps
1 nc
T F dt
T c
%%IncludeResource: font Courier
/_F0 /VIM-latin1 /Courier ref
/F0 10 /_F0 ffs
%%IncludeResource: font Courier-Bold
/_F1 /VIM-latin1 /Courier-Bold ref
/F1 10 /_F1 ffs
%%IncludeResource: font Courier-Oblique
/_F2 /VIM-latin1 /Courier-Oblique ref
/F2 10 /_F2 ffs
%%IncludeResource: font Courier-BoldOblique
/_F3 /VIM-latin1 /Courier-BoldOblique ref
/F3 10 /_F3 ffs
/UO -1 d
/UW 0.5 d
/BO -2.5 d
%%EndSetup
%%Page: 1 1
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(linear_classifier.py                                                          Page 1)59.5 792.4 ms
F0 sf
0.753 0 0.753 r
(from)59.5 772.4 ms
0 g
( __future__ )s
0.753 0 0.753 r
(import)s
0 g
( print_function)s
0.753 0 0.753 r
(import)59.5 752.4 ms
0 g
( numpy )s
0 0.502 0 r
(as)s
0 g
( np)s
0.753 0 0.753 r
(from)59.5 742.4 ms
0 g
( hmwk5_2.classifiers.linear_svm )s
0.753 0 0.753 r
(import)s
0 g
( *)s
0.753 0 0.753 r
(from)59.5 732.4 ms
0 g
( hmwk5_2.classifiers.softmax )s
0.753 0 0.753 r
(import)s
0 g
( *)s
0 0.502 0 r
(class)59.5 712.4 ms
0 g
( )s
0.502 0.502 0 r
(LinearClassifier)s
0 g
(\()s
0.502 0.502 0 r
(object)s
0 g
(\):)s
(  )59.5 692.4 ms
0 0.502 0 r
(def)s
0 g
( )s
0.502 0.502 0 r
(__init__)s
0 g
(\(self\):)s
(    self.W = )59.5 682.4 ms
0.502 0.502 0 r
(None)s
0 g
(  )59.5 662.4 ms
0 0.502 0 r
(def)s
0 g
( )s
0.502 0.502 0 r
(train)s
0 g
(\(self, X, y, learning_rate=)s
0 0 0.753 r
(1e-3)s
0 g
(, reg=)s
0 0 0.753 r
(1e-5)s
0 g
(, num_iters=)s
0 0 0.753 r
(100)s
0 g
(,)s
(            batch_size=)59.5 652.4 ms
0 0 0.753 r
(200)s
0 g
(, verbose=)s
0.502 0.502 0 r
(False)s
0 g
(\):)s
(    )59.5 642.4 ms
0 0 0.753 r
(""")s
(    Train this linear classifier using stochastic gradient descent.)59.5 632.4 ms
(    Inputs:)59.5 612.4 ms
(    - X: A numpy array of shape \(N, D\) containing training data; there are N)59.5 602.4 ms
(      training samples each of dimension D.)59.5 592.4 ms
(    - y: A numpy array of shape \(N,\) containing training labels; y[i] = c)59.5 582.4 ms
(      means that X[i] has label 0 <= c < C for C classes.)59.5 572.4 ms
(    - learning_rate: \(float\) learning rate for optimization.)59.5 562.4 ms
(    - reg: \(float\) regularization strength.)59.5 552.4 ms
(    - num_iters: \(integer\) number of steps to take when optimizing)59.5 542.4 ms
(    - batch_size: \(integer\) number of training examples to use at each step.)59.5 532.4 ms
(    - verbose: \(boolean\) If true, print progress during optimization.)59.5 522.4 ms
(    Outputs:)59.5 502.4 ms
(    A list containing the value of the loss function at each training iteration.)59.5 492.4 ms
(    """)59.5 482.4 ms
0 g
(    num_train, dim = X.shape)59.5 472.4 ms
(    num_classes = np.max\(y\) + )59.5 462.4 ms
0 0 0.753 r
(1)s
0 g
( )s
0.753 0 0 r
(# assume y takes values 0...K-1 where K is number of)s
( classes)59.5 452.4 ms
0 g
(    )59.5 442.4 ms
0 0.502 0 r
(if)s
0 g
( self.W )s
0 0.502 0 r
(is)s
0 g
( )s
0.502 0.502 0 r
(None)s
0 g
(:)s
(      )59.5 432.4 ms
0.753 0 0 r
(# lazily initialize W)s
0 g
(      self.W = )59.5 422.4 ms
0 0 0.753 r
(0.001)s
0 g
( * np.random.randn\(dim, num_classes\))s
(    )59.5 402.4 ms
0.753 0 0 r
(# Run stochastic gradient descent to optimize W)s
0 g
(    loss_history = [])59.5 392.4 ms
(    )59.5 382.4 ms
0 0.502 0 r
(for)s
0 g
( it )s
0 0.502 0 r
(in)s
0 g
( )s
0.502 0.502 0 r
(range)s
0 g
(\(num_iters\):)s
(      X_batch = )59.5 372.4 ms
0.502 0.502 0 r
(None)s
0 g
(      y_batch = )59.5 362.4 ms
0.502 0.502 0 r
(None)s
0 g
(      )59.5 342.4 ms
0.753 0 0 r
(#########################################################################)s
0 g
(      )59.5 332.4 ms
0.753 0 0 r
(# )s
0 g
(TODO)s
0.753 0 0 r
(:                                                                 #)s
0 g
(      )59.5 322.4 ms
0.753 0 0 r
(# Sample batch_size elements from the training data and their           #)s
0 g
(      )59.5 312.4 ms
0.753 0 0 r
(# corresponding labels to use in this round of gradient descent.        #)s
0 g
(      )59.5 302.4 ms
0.753 0 0 r
(# Store the data in X_batch and their corresponding labels in           #)s
0 g
(      )59.5 292.4 ms
0.753 0 0 r
(# y_batch; after sampling X_batch should have shape \(dim, batch_size\)   #)s
0 g
(      )59.5 282.4 ms
0.753 0 0 r
(# and y_batch should have shape \(batch_size,\)                           #)s
0 g
(      )59.5 272.4 ms
0.753 0 0 r
(#                                                                       #)s
0 g
(      )59.5 262.4 ms
0.753 0 0 r
(# Hint: Use np.random.choice to generate indices. Sampling with         #)s
0 g
(      )59.5 252.4 ms
0.753 0 0 r
(# replacement is faster than sampling without replacement.              #)s
0 g
(      )59.5 242.4 ms
0.753 0 0 r
(#########################################################################)s
0 g
(      indxs = np.random.choice\(num_train,batch_size\))59.5 232.4 ms
(      X_batch = X[indxs])59.5 222.4 ms
(      y_batch = y[indxs])59.5 212.4 ms
(      )59.5 202.4 ms
0 0.502 0 r
(pass)s
0 g
(      )59.5 192.4 ms
0.753 0 0 r
(#########################################################################)s
0 g
(      )59.5 182.4 ms
0.753 0 0 r
(#                       END OF YOUR CODE                                #)s
0 g
(      )59.5 172.4 ms
0.753 0 0 r
(#########################################################################)s
0 g
(      )59.5 152.4 ms
0.753 0 0 r
(# evaluate loss and gradient)s
0 g
(      loss, grad = self.loss\(X_batch, y_batch, reg\))59.5 142.4 ms
(      loss_history.append\(loss\))59.5 132.4 ms
(      )59.5 112.4 ms
0.753 0 0 r
(# perform parameter update)s
0 g
(      )59.5 102.4 ms
0.753 0 0 r
(#########################################################################)s
0 g
(      )59.5 92.4 ms
0.753 0 0 r
(# )s
0 g
(TODO)s
0.753 0 0 r
(:                                                                 #)s
0 g
(      )59.5 82.4 ms
0.753 0 0 r
(# Update the weights using the gradient and the learning rate.          #)s
0 g
(      )59.5 72.4 ms
0.753 0 0 r
(#########################################################################)s
0 g
(      self.W -= learning_rate*grad)59.5 62.4 ms
(      )59.5 52.4 ms
0 0.502 0 r
(pass)s
re sp
%%PageTrailer
%%Page: 2 2
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(linear_classifier.py                                                          Page 2)59.5 792.4 ms
F0 sf
(      )59.5 772.4 ms
0.753 0 0 r
(#########################################################################)s
0 g
(      )59.5 762.4 ms
0.753 0 0 r
(#                       END OF YOUR CODE                                #)s
0 g
(      )59.5 752.4 ms
0.753 0 0 r
(#########################################################################)s
0 g
(      )59.5 732.4 ms
0 0.502 0 r
(if)s
0 g
( verbose )s
0 0.502 0 r
(and)s
0 g
( it % )s
0 0 0.753 r
(100)s
0 g
( == )s
0 0 0.753 r
(0)s
0 g
(:)s
(        )59.5 722.4 ms
0.502 0.502 0 r
(print)s
0 g
(\()s
0 0 0.753 r
('iteration %d / %d: loss %f')s
0 g
( % \(it, num_iters, loss\)\))s
(    )59.5 702.4 ms
0 0.502 0 r
(return)s
0 g
( loss_history)s
(  )59.5 682.4 ms
0 0.502 0 r
(def)s
0 g
( )s
0.502 0.502 0 r
(predict)s
0 g
(\(self, X\):)s
(    )59.5 672.4 ms
0 0 0.753 r
(""")s
(    Use the trained weights of this linear classifier to predict labels for)59.5 662.4 ms
(    data points.)59.5 652.4 ms
(    Inputs:)59.5 632.4 ms
(    - X: A numpy array of shape \(N, D\) containing training data; there are N)59.5 622.4 ms
(      training samples each of dimension D.)59.5 612.4 ms
(    Returns:)59.5 592.4 ms
(    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional)59.5 582.4 ms
(      array of length N, and each element is an integer giving the predicted)59.5 572.4 ms
(      class.)59.5 562.4 ms
(    """)59.5 552.4 ms
0 g
(    y_pred = np.zeros\(X.shape[)59.5 542.4 ms
0 0 0.753 r
(0)s
0 g
(]\))s
(    )59.5 532.4 ms
0.753 0 0 r
(###########################################################################)s
0 g
(    )59.5 522.4 ms
0.753 0 0 r
(# )s
0 g
(TODO)s
0.753 0 0 r
(:                                                                   #)s
0 g
(    )59.5 512.4 ms
0.753 0 0 r
(# Implement this method. Store the predicted labels in y_pred.            #)s
0 g
(    )59.5 502.4 ms
0.753 0 0 r
(###########################################################################)s
0 g
(    y_pred = np.argmax\(np.dot\(X,self.W\),axis = )59.5 492.4 ms
0 0 0.753 r
(1)s
0 g
(\))s
(    )59.5 482.4 ms
0 0.502 0 r
(pass)s
0 g
(    )59.5 472.4 ms
0.753 0 0 r
(###########################################################################)s
0 g
(    )59.5 462.4 ms
0.753 0 0 r
(#                           END OF YOUR CODE                              #)s
0 g
(    )59.5 452.4 ms
0.753 0 0 r
(###########################################################################)s
0 g
(    )59.5 442.4 ms
0 0.502 0 r
(return)s
0 g
( y_pred)s
(  )59.5 432.4 ms
(  )59.5 422.4 ms
0 0.502 0 r
(def)s
0 g
( )s
0.502 0.502 0 r
(loss)s
0 g
(\(self, X_batch, y_batch, reg\):)s
(    )59.5 412.4 ms
0 0 0.753 r
(""")s
(    Compute the loss function and its derivative. )59.5 402.4 ms
(    Subclasses will override this.)59.5 392.4 ms
(    Inputs:)59.5 372.4 ms
(    - X_batch: A numpy array of shape \(N, D\) containing a minibatch of N)59.5 362.4 ms
(      data points; each point has dimension D.)59.5 352.4 ms
(    - y_batch: A numpy array of shape \(N,\) containing labels for the minibatch.)59.5 342.4 ms
(    - reg: \(float\) regularization strength.)59.5 332.4 ms
(    Returns: A tuple containing:)59.5 312.4 ms
(    - loss as a single float)59.5 302.4 ms
(    - gradient with respect to self.W; an array of the same shape as W)59.5 292.4 ms
(    """)59.5 282.4 ms
0 g
(    )59.5 272.4 ms
0 0.502 0 r
(pass)s
(class)59.5 242.4 ms
0 g
( )s
0.502 0.502 0 r
(LinearSVM)s
0 g
(\(LinearClassifier\):)s
(  )59.5 232.4 ms
0 0 0.753 r
(""" A subclass that uses the Multiclass SVM loss function """)s
0 g
(  )59.5 212.4 ms
0 0.502 0 r
(def)s
0 g
( )s
0.502 0.502 0 r
(loss)s
0 g
(\(self, X_batch, y_batch, reg\):)s
(    )59.5 202.4 ms
0 0.502 0 r
(return)s
0 g
( svm_loss_vectorized\(self.W, X_batch, y_batch, reg\))s
0 0.502 0 r
(class)59.5 172.4 ms
0 g
( )s
0.502 0.502 0 r
(Softmax)s
0 g
(\(LinearClassifier\):)s
(  )59.5 162.4 ms
0 0 0.753 r
(""" A subclass that uses the Softmax + Cross-entropy loss function """)s
0 g
(  )59.5 142.4 ms
0 0.502 0 r
(def)s
0 g
( )s
0.502 0.502 0 r
(loss)s
0 g
(\(self, X_batch, y_batch, reg\):)s
(    )59.5 132.4 ms
0 0.502 0 r
(return)s
0 g
( softmax_loss_vectorized\(self.W, X_batch, y_batch, reg\))s
re sp
%%PageTrailer
%%Trailer
%%Pages: 2
%%EOF
